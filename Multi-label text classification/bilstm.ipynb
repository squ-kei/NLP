{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Load packages"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport sklearn\nimport zipfile\nimport string\nimport re\nimport nltk\n\nimport tensorflow as tf","execution_count":1,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load data"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"with zipfile.ZipFile(\"../input/jigsaw-toxic-comment-classification-challenge/train.csv.zip\",\"r\") as zip_ref:\n    zip_ref.extractall(\"./\")\n\ndf = pd.read_csv(\"train.csv\")\ndf.head()","execution_count":2,"outputs":[{"output_type":"execute_result","execution_count":2,"data":{"text/plain":"                 id                                       comment_text  toxic  \\\n0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n\n   severe_toxic  obscene  threat  insult  identity_hate  \n0             0        0       0       0              0  \n1             0        0       0       0              0  \n2             0        0       0       0              0  \n3             0        0       0       0              0  \n4             0        0       0       0              0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>comment_text</th>\n      <th>toxic</th>\n      <th>severe_toxic</th>\n      <th>obscene</th>\n      <th>threat</th>\n      <th>insult</th>\n      <th>identity_hate</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0000997932d777bf</td>\n      <td>Explanation\\nWhy the edits made under my usern...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>000103f0d9cfb60f</td>\n      <td>D'aww! He matches this background colour I'm s...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>000113f07ec002fd</td>\n      <td>Hey man, I'm really not trying to edit war. It...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0001b41b1c6bb37e</td>\n      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0001d958c54c6e35</td>\n      <td>You, sir, are my hero. Any chance you remember...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"# Few preprocessing\nA few preprocessing before tokenization. Tensorflow Keras' built-in tokenizer already include some preprocessing, here we do some preprocessing that are not included but suitable for a deep learning model approach.\n## Contractions"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install contractions","execution_count":3,"outputs":[{"output_type":"stream","text":"Collecting contractions\n  Downloading contractions-0.0.43-py2.py3-none-any.whl (6.0 kB)\nCollecting textsearch\n  Downloading textsearch-0.0.17-py2.py3-none-any.whl (7.5 kB)\nRequirement already satisfied: Unidecode in /opt/conda/lib/python3.7/site-packages (from textsearch->contractions) (1.1.1)\nCollecting pyahocorasick\n  Downloading pyahocorasick-1.4.0.tar.gz (312 kB)\n\u001b[K     |████████████████████████████████| 312 kB 6.2 MB/s eta 0:00:01\n\u001b[?25hBuilding wheels for collected packages: pyahocorasick\n  Building wheel for pyahocorasick (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pyahocorasick: filename=pyahocorasick-1.4.0-cp37-cp37m-linux_x86_64.whl size=99057 sha256=23753ea8e40036ff7fab63e7406dcc7477e209dbd76931113e9eaf1d82229c5a\n  Stored in directory: /root/.cache/pip/wheels/9b/6b/f7/62dc8caf183b125107209c014e78c340a0b4b7b392c23c2db4\nSuccessfully built pyahocorasick\nInstalling collected packages: pyahocorasick, textsearch, contractions\nSuccessfully installed contractions-0.0.43 pyahocorasick-1.4.0 textsearch-0.0.17\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import contractions\n\ndf[\"text_clean\"] = df[\"comment_text\"].apply(lambda x: contractions.fix(x))","execution_count":4,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Remove URL and HTTP tags"},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_URL(text):\n    \"\"\"\n        Remove URLs from a sample string\n    \"\"\"\n    return re.sub(r\"https?://\\S+|www\\.\\S+\", \"\", text)\n\n\ndef remove_html(text):\n    \"\"\"\n        Remove the html in sample text\n    \"\"\"\n    html = re.compile(r\"<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});\")\n    return re.sub(html, \"\", text)\n\ndf[\"text_clean\"] = df[\"text_clean\"].apply(lambda x: remove_URL(x))\ndf[\"text_clean\"] = df[\"text_clean\"].apply(lambda x: remove_html(x))","execution_count":5,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Remove Non-ASCI"},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_non_ascii(text):\n    \"\"\"\n        Remove non-ASCII characters \n    \"\"\"\n    return re.sub(r'[^\\x00-\\x7f]',r'', text)\n\ndf[\"text_clean\"] = df[\"text_clean\"].apply(lambda x: remove_non_ascii(x))","execution_count":6,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Remove special characters"},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_special_characters(text):\n    \"\"\"\n        Remove special special characters, including symbols, emojis, and other graphic characters\n    \"\"\"\n    emoji_pattern = re.compile(\n        '['\n        u'\\U0001F600-\\U0001F64F'  # emoticons\n        u'\\U0001F300-\\U0001F5FF'  # symbols & pictographs\n        u'\\U0001F680-\\U0001F6FF'  # transport & map symbols\n        u'\\U0001F1E0-\\U0001F1FF'  # flags (iOS)\n        u'\\U00002702-\\U000027B0'\n        u'\\U000024C2-\\U0001F251'\n        ']+',\n        flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\ndf[\"text_clean\"] = df[\"text_clean\"].apply(lambda x: remove_special_characters(x))","execution_count":7,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Split and store data"},{"metadata":{"trusted":true},"cell_type":"code","source":"label_col = list(df.columns[2:8])\nx = df['text_clean'].values\ny = df[label_col].values\n\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2,random_state=42)","execution_count":8,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Tokenization"},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.preprocessing.text import Tokenizer\nvocab_size = 20000\n\ntokenizer = Tokenizer(num_words=vocab_size)\ntokenizer.fit_on_texts(list(x))","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(tokenizer.word_index)","execution_count":10,"outputs":[{"output_type":"execute_result","execution_count":10,"data":{"text/plain":"192394"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer.word_index","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**A note here is although our vocab_size is set to 20000, the tokenizer still keeps 192395 in the word_index.\nword_index is computed the same way no matter how many most frequent words you will use later. So when you call any transformative method - Tokenizer will use only the vocab_size most common words and at the same time, it will keep the counter of all words - even when it's obvious that it will not use it later.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train = tokenizer.texts_to_sequences(x_train)\nx_test = tokenizer.texts_to_sequences(x_test)","execution_count":11,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Add padding"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.sequence import pad_sequences\n\nmaxlen = 150\nx_train = pad_sequences(x_train, padding='post', maxlen=maxlen)\nx_test = pad_sequences(x_test, padding='post', maxlen=maxlen)","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x[0]","execution_count":13,"outputs":[{"output_type":"execute_result","execution_count":13,"data":{"text/plain":"'Explanation\\nWhy the edits made under my username Hardcore Metallica Fan were reverted? They were not vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please do not remove the template from the talk page since I am retired now.89.205.38.27'"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"example = tokenizer.texts_to_sequences([x[0]])\nexample = pad_sequences(example, padding='post', maxlen=maxlen)\nexample","execution_count":14,"outputs":[{"output_type":"execute_result","execution_count":14,"data":{"text/plain":"array([[  661,    75,     1,   123,   127,   172,    29,   644,  4448,\n        11794,  1068,    83,   313,    48,    83,    10, 11223,    53,\n         6775,    15,    60,  2699,   144,     3,  2863,    36,   114,\n         1176, 15683,  2756,     5,    47,    20,    10,   232,     1,\n          349,    32,     1,    40,    28,   140,     3,    37,  3372,\n           87,  3003,  4523,  2235,   997,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0]], dtype=int32)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer.sequences_to_texts(example)","execution_count":15,"outputs":[{"output_type":"execute_result","execution_count":15,"data":{"text/plain":"['explanation why the edits made under my username hardcore metallica fan were reverted they were not vandalisms just closure on some gas after i voted at new york dolls fac and please do not remove the template from the talk page since i am retired now 89 205 38 27']"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"# Load pretrained embedding"},{"metadata":{"trusted":true},"cell_type":"code","source":"embeddings_dictionary = dict()\n\nglove_file = open('../input/glove6b100dtxt/glove.6B.100d.txt', encoding=\"utf8\")\n\nfor line in glove_file:\n    records = line.split()\n    word = records[0]\n    vector_dimensions = np.asarray(records[1:], dtype='float32')\n    embeddings_dictionary[word] = vector_dimensions\nglove_file.close()\n\n\n\ndef create_glove(word_index,embeddings_index):\n    emb_mean,emb_std = -0.005838499,0.48782197\n    all_embs = np.stack(embeddings_index.values())\n    embed_size = all_embs.shape[1]\n    nb_words = min(vocab_size, len(word_index))\n    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n    count_found = nb_words\n    for word, i in word_index.items():\n        if i >= vocab_size: continue\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None:\n            embedding_matrix[i] =  embedding_vector\n        else:\n                count_found-=1\n    print(\"Got embedding for \",count_found,\" words.\")\n    return embedding_matrix\n","execution_count":16,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_matrix = create_glove(tokenizer.word_index,embeddings_dictionary)","execution_count":17,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3331: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n  exec(code_obj, self.user_global_ns, self.user_ns)\n","name":"stderr"},{"output_type":"stream","text":"Got embedding for  18783  words.\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"all(embedding_matrix[1] == embeddings_dictionary['the'])","execution_count":18,"outputs":[{"output_type":"execute_result","execution_count":18,"data":{"text/plain":"True"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"all(embedding_matrix[tokenizer.word_index['which']] == embeddings_dictionary['which'])","execution_count":19,"outputs":[{"output_type":"execute_result","execution_count":19,"data":{"text/plain":"True"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"# Model building with pretrained Glove embeddings"},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\nfrom tensorflow.keras.layers import Bidirectional, GlobalMaxPool1D\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras import initializers, regularizers, constraints, optimizers, layers\nimport tensorflow_addons as tfa\n\ndef model_add():\n    inputs = Input(shape=(maxlen, ))\n    x = Embedding(vocab_size, 100, weights=[embedding_matrix], trainable=False)(inputs)\n    x = Bidirectional(LSTM(50))(x)\n    x = Dropout(0.1)(x)\n    x = Dense(50, activation=\"relu\")(x)\n    x = Dropout(0.1)(x)\n    outputs = Dense(6, activation=\"sigmoid\")(x)\n    model = Model(inputs=inputs, outputs=outputs)\n    f1 = tfa.metrics.F1Score(num_classes=1, average='micro',threshold=0.5)\n    model.compile(loss='binary_crossentropy',\n                  optimizer='adam',\n                  metrics=['acc',f1])\n    return model\nmodel_w_glove = model_add()\nprint(model_w_glove.summary())","execution_count":20,"outputs":[{"output_type":"stream","text":"Model: \"functional_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_1 (InputLayer)         [(None, 150)]             0         \n_________________________________________________________________\nembedding (Embedding)        (None, 150, 100)          2000000   \n_________________________________________________________________\nbidirectional (Bidirectional (None, 100)               60400     \n_________________________________________________________________\ndropout (Dropout)            (None, 100)               0         \n_________________________________________________________________\ndense (Dense)                (None, 50)                5050      \n_________________________________________________________________\ndropout_1 (Dropout)          (None, 50)                0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 6)                 306       \n=================================================================\nTotal params: 2,065,756\nTrainable params: 65,756\nNon-trainable params: 2,000,000\n_________________________________________________________________\nNone\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.callbacks import EarlyStopping, ModelCheckpoint\nes = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=20)\nhistory = model_w_glove.fit(x_train, y_train, batch_size=32, epochs=5, validation_data=(x_test,y_test), callbacks=es)","execution_count":21,"outputs":[{"output_type":"stream","text":"Epoch 1/5\n3990/3990 [==============================] - 307s 77ms/step - loss: 0.0655 - acc: 0.9344 - f1_score: 0.6368 - val_loss: 0.0567 - val_acc: 0.9941 - val_f1_score: 0.6504\nEpoch 2/5\n3990/3990 [==============================] - 300s 75ms/step - loss: 0.0521 - acc: 0.9828 - f1_score: 0.7060 - val_loss: 0.0502 - val_acc: 0.9930 - val_f1_score: 0.7220\nEpoch 3/5\n3990/3990 [==============================] - 308s 77ms/step - loss: 0.0479 - acc: 0.9836 - f1_score: 0.7319 - val_loss: 0.0482 - val_acc: 0.9938 - val_f1_score: 0.7214\nEpoch 4/5\n3990/3990 [==============================] - 307s 77ms/step - loss: 0.0454 - acc: 0.9860 - f1_score: 0.7458 - val_loss: 0.0467 - val_acc: 0.9929 - val_f1_score: 0.7426\nEpoch 5/5\n3990/3990 [==============================] - 307s 77ms/step - loss: 0.0431 - acc: 0.9824 - f1_score: 0.7575 - val_loss: 0.0460 - val_acc: 0.9938 - val_f1_score: 0.7380\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred2 = model_w_glove.predict(x_test)\ny_pred2 = y_pred2 > 0.5\nfrom sklearn.metrics import f1_score\nf1_score(y_test, y_pred2, average='micro')","execution_count":22,"outputs":[{"output_type":"execute_result","execution_count":22,"data":{"text/plain":"0.7380073800738006"},"metadata":{}}]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}