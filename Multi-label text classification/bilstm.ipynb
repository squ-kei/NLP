{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Load packages"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport sklearn\nimport zipfile\nimport string\nimport re\nimport nltk\n\nimport tensorflow as tf","execution_count":1,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load data"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"with zipfile.ZipFile(\"../input/jigsaw-toxic-comment-classification-challenge/train.csv.zip\",\"r\") as zip_ref:\n    zip_ref.extractall(\"./\")\n\ndf = pd.read_csv(\"train.csv\")\ndf.head()","execution_count":2,"outputs":[{"output_type":"execute_result","execution_count":2,"data":{"text/plain":"                 id                                       comment_text  toxic  \\\n0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n\n   severe_toxic  obscene  threat  insult  identity_hate  \n0             0        0       0       0              0  \n1             0        0       0       0              0  \n2             0        0       0       0              0  \n3             0        0       0       0              0  \n4             0        0       0       0              0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>comment_text</th>\n      <th>toxic</th>\n      <th>severe_toxic</th>\n      <th>obscene</th>\n      <th>threat</th>\n      <th>insult</th>\n      <th>identity_hate</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0000997932d777bf</td>\n      <td>Explanation\\nWhy the edits made under my usern...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>000103f0d9cfb60f</td>\n      <td>D'aww! He matches this background colour I'm s...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>000113f07ec002fd</td>\n      <td>Hey man, I'm really not trying to edit war. It...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0001b41b1c6bb37e</td>\n      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0001d958c54c6e35</td>\n      <td>You, sir, are my hero. Any chance you remember...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"# Few preprocessing\nA few preprocessing before tokenization. Tensorflow Keras' built-in tokenizer already include some preprocessing, here we do some preprocessing that are not included but suitable for a deep learning model approach.\n## Contractions"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install contractions","execution_count":3,"outputs":[{"output_type":"stream","text":"Collecting contractions\n  Downloading contractions-0.0.43-py2.py3-none-any.whl (6.0 kB)\nCollecting textsearch\n  Downloading textsearch-0.0.17-py2.py3-none-any.whl (7.5 kB)\nRequirement already satisfied: Unidecode in /opt/conda/lib/python3.7/site-packages (from textsearch->contractions) (1.1.1)\nCollecting pyahocorasick\n  Downloading pyahocorasick-1.4.0.tar.gz (312 kB)\n\u001b[K     |████████████████████████████████| 312 kB 883 kB/s eta 0:00:01\n\u001b[?25hBuilding wheels for collected packages: pyahocorasick\n  Building wheel for pyahocorasick (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pyahocorasick: filename=pyahocorasick-1.4.0-cp37-cp37m-linux_x86_64.whl size=99062 sha256=14a7a1a3230547465041cf1d70879e5078bade3d91a10da2d828738475498c61\n  Stored in directory: /root/.cache/pip/wheels/9b/6b/f7/62dc8caf183b125107209c014e78c340a0b4b7b392c23c2db4\nSuccessfully built pyahocorasick\nInstalling collected packages: pyahocorasick, textsearch, contractions\nSuccessfully installed contractions-0.0.43 pyahocorasick-1.4.0 textsearch-0.0.17\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import contractions\n\ndf[\"text_clean\"] = df[\"comment_text\"].apply(lambda x: contractions.fix(x))","execution_count":4,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Remove URL and HTTP tags"},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_URL(text):\n    \"\"\"\n        Remove URLs from a sample string\n    \"\"\"\n    return re.sub(r\"https?://\\S+|www\\.\\S+\", \"\", text)\n\n\ndef remove_html(text):\n    \"\"\"\n        Remove the html in sample text\n    \"\"\"\n    html = re.compile(r\"<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});\")\n    return re.sub(html, \"\", text)\n\ndf[\"text_clean\"] = df[\"text_clean\"].apply(lambda x: remove_URL(x))\ndf[\"text_clean\"] = df[\"text_clean\"].apply(lambda x: remove_html(x))","execution_count":5,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Remove Non-ASCI"},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_non_ascii(text):\n    \"\"\"\n        Remove non-ASCII characters \n    \"\"\"\n    return re.sub(r'[^\\x00-\\x7f]',r'', text)\n\ndf[\"text_clean\"] = df[\"text_clean\"].apply(lambda x: remove_non_ascii(x))","execution_count":6,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Remove special characters"},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_special_characters(text):\n    \"\"\"\n        Remove special special characters, including symbols, emojis, and other graphic characters\n    \"\"\"\n    emoji_pattern = re.compile(\n        '['\n        u'\\U0001F600-\\U0001F64F'  # emoticons\n        u'\\U0001F300-\\U0001F5FF'  # symbols & pictographs\n        u'\\U0001F680-\\U0001F6FF'  # transport & map symbols\n        u'\\U0001F1E0-\\U0001F1FF'  # flags (iOS)\n        u'\\U00002702-\\U000027B0'\n        u'\\U000024C2-\\U0001F251'\n        ']+',\n        flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\ndf[\"text_clean\"] = df[\"text_clean\"].apply(lambda x: remove_special_characters(x))","execution_count":7,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Split and store data"},{"metadata":{"trusted":true},"cell_type":"code","source":"label_col = list(df.columns[2:8])\nx = df['text_clean'].values\ny = df[label_col].values\n\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2,random_state=42)","execution_count":8,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Tokenization"},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.preprocessing.text import Tokenizer\nvocab_size = 20000\n\ntokenizer = Tokenizer(num_words=vocab_size)\ntokenizer.fit_on_texts(list(x))","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(tokenizer.word_index)","execution_count":10,"outputs":[{"output_type":"execute_result","execution_count":10,"data":{"text/plain":"192394"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer.word_index","execution_count":19,"outputs":[{"output_type":"execute_result","execution_count":19,"data":{"text/plain":"{'the': 1,\n 'to': 2,\n 'i': 3,\n 'of': 4,\n 'and': 5,\n 'you': 6,\n 'a': 7,\n 'is': 8,\n 'that': 9,\n 'not': 10,\n 'it': 11,\n 'in': 12,\n 'for': 13,\n 'this': 14,\n 'on': 15,\n 'have': 16,\n 'are': 17,\n 'be': 18,\n 'as': 19,\n 'do': 20,\n 'your': 21,\n 'with': 22,\n 'if': 23,\n 'was': 24,\n 'article': 25,\n 'or': 26,\n 'but': 27,\n 'page': 28,\n 'my': 29,\n 'an': 30,\n 'wikipedia': 31,\n 'from': 32,\n 'by': 33,\n 'will': 34,\n 'can': 35,\n 'at': 36,\n 'am': 37,\n 'me': 38,\n 'about': 39,\n 'talk': 40,\n 'so': 41,\n 'would': 42,\n 'what': 43,\n 'there': 44,\n 'all': 45,\n 'has': 46,\n 'please': 47,\n 'they': 48,\n 'no': 49,\n 'he': 50,\n 'one': 51,\n 'like': 52,\n 'just': 53,\n 'we': 54,\n 'which': 55,\n 'should': 56,\n 'any': 57,\n 'been': 58,\n 'more': 59,\n 'some': 60,\n 'here': 61,\n 'who': 62,\n 'other': 63,\n 'see': 64,\n 'did': 65,\n 'also': 66,\n 'because': 67,\n 'his': 68,\n 'think': 69,\n 'know': 70,\n 'how': 71,\n 'does': 72,\n 'edit': 73,\n 'people': 74,\n 'why': 75,\n 'up': 76,\n 'only': 77,\n 'out': 78,\n 'use': 79,\n 'articles': 80,\n 'when': 81,\n 'then': 82,\n 'were': 83,\n 'time': 84,\n 'may': 85,\n 'them': 86,\n 'now': 87,\n 'being': 88,\n 'their': 89,\n 'than': 90,\n 'thanks': 91,\n 'user': 92,\n 'even': 93,\n 'get': 94,\n 'make': 95,\n 'could': 96,\n 'had': 97,\n 'good': 98,\n 'well': 99,\n 'very': 100,\n 'information': 101,\n 'want': 102,\n 'its': 103,\n 'deletion': 104,\n 'such': 105,\n 'sources': 106,\n 'way': 107,\n 'name': 108,\n 'these': 109,\n 'first': 110,\n 'wp': 111,\n 'help': 112,\n 'pages': 113,\n 'new': 114,\n 'image': 115,\n 'editing': 116,\n 'source': 117,\n 'go': 118,\n 'need': 119,\n 'section': 120,\n 'say': 121,\n 'again': 122,\n 'edits': 123,\n 'where': 124,\n 'thank': 125,\n 'fuck': 126,\n 'made': 127,\n 'many': 128,\n 'much': 129,\n 'used': 130,\n 'really': 131,\n 'discussion': 132,\n 'deleted': 133,\n 'most': 134,\n 'same': 135,\n 'find': 136,\n 'into': 137,\n 'work': 138,\n 'those': 139,\n 'since': 140,\n 'point': 141,\n 'right': 142,\n 'before': 143,\n 'after': 144,\n 'add': 145,\n 'read': 146,\n 'look': 147,\n 'over': 148,\n 'going': 149,\n 'him': 150,\n 'take': 151,\n 'us': 152,\n 'two': 153,\n 'still': 154,\n 'back': 155,\n 'someone': 156,\n 'fact': 157,\n 'too': 158,\n 'hi': 159,\n 'link': 160,\n 'list': 161,\n 'own': 162,\n 'said': 163,\n 'something': 164,\n 'blocked': 165,\n 'stop': 166,\n '2': 167,\n 'without': 168,\n 'content': 169,\n '1': 170,\n 'block': 171,\n 'under': 172,\n 'our': 173,\n 'added': 174,\n 'utc': 175,\n 'history': 176,\n 'editors': 177,\n 'removed': 178,\n 'another': 179,\n 'her': 180,\n 'let': 181,\n 'might': 182,\n 'welcome': 183,\n 'she': 184,\n 'note': 185,\n 'however': 186,\n 'place': 187,\n 'sure': 188,\n 'free': 189,\n 'case': 190,\n 'never': 191,\n 'done': 192,\n 'vandalism': 193,\n 'reason': 194,\n 'put': 195,\n 'wiki': 196,\n 'comment': 197,\n 'personal': 198,\n 'better': 199,\n 'yourself': 200,\n 'using': 201,\n 'seems': 202,\n 'ask': 203,\n 'actually': 204,\n 'question': 205,\n 'while': 206,\n 'off': 207,\n 'feel': 208,\n 'anything': 209,\n 'believe': 210,\n 'links': 211,\n 'person': 212,\n 'things': 213,\n 'both': 214,\n 'best': 215,\n 'comments': 216,\n 'policy': 217,\n 'part': 218,\n 'hope': 219,\n 'against': 220,\n 'keep': 221,\n 'already': 222,\n 'thing': 223,\n 'questions': 224,\n '3': 225,\n 'nothing': 226,\n 'change': 227,\n 'wrong': 228,\n 'though': 229,\n 'subject': 230,\n 'problem': 231,\n 'remove': 232,\n 'little': 233,\n 'copyright': 234,\n 'tag': 235,\n 'trying': 236,\n 'long': 237,\n 'must': 238,\n 'understand': 239,\n 'above': 240,\n 'speedy': 241,\n 'anyone': 242,\n 'few': 243,\n 'issue': 244,\n 'last': 245,\n 'give': 246,\n 'world': 247,\n 'others': 248,\n 'editor': 249,\n 'sorry': 250,\n 'agree': 251,\n 'reliable': 252,\n 'rather': 253,\n 'years': 254,\n 'fair': 255,\n 'different': 256,\n 'making': 257,\n 'english': 258,\n 'come': 259,\n 'style': 260,\n 'reference': 261,\n 'text': 262,\n 'got': 263,\n 'references': 264,\n 'mean': 265,\n 'try': 266,\n 'non': 267,\n 'continue': 268,\n 'doing': 269,\n 'found': 270,\n 'great': 271,\n 'leave': 272,\n 'word': 273,\n 'says': 274,\n 'original': 275,\n 'probably': 276,\n 'state': 277,\n 'adding': 278,\n 'every': 279,\n 'check': 280,\n 'simply': 281,\n 'site': 282,\n 'day': 283,\n 'created': 284,\n 'hello': 285,\n 'life': 286,\n 'either': 287,\n 'consensus': 288,\n 'top': 289,\n 'show': 290,\n 'post': 291,\n 'ip': 292,\n 'least': 293,\n 'delete': 294,\n 'else': 295,\n 'e': 296,\n 'yes': 297,\n 'far': 298,\n 'notable': 299,\n 'enough': 300,\n 'request': 301,\n 'etc': 302,\n 'war': 303,\n 'example': 304,\n 'opinion': 305,\n 'called': 306,\n 'contributions': 307,\n 'view': 308,\n 'around': 309,\n 'through': 310,\n 'between': 311,\n 'yet': 312,\n 'reverted': 313,\n 'write': 314,\n 'shit': 315,\n 'matter': 316,\n 'admin': 317,\n 'down': 318,\n 'book': 319,\n 'real': 320,\n 're': 321,\n 'thought': 322,\n 'given': 323,\n 'material': 324,\n 'account': 325,\n 'bad': 326,\n 'users': 327,\n 'images': 328,\n 'having': 329,\n 'encyclopedia': 330,\n 'clearly': 331,\n 'message': 332,\n 'needs': 333,\n 'support': 334,\n 'lot': 335,\n 'old': 336,\n 'evidence': 337,\n 'ever': 338,\n 'tell': 339,\n 'maybe': 340,\n 'revert': 341,\n 's': 342,\n 'seem': 343,\n 'language': 344,\n 'instead': 345,\n 'correct': 346,\n 'clear': 347,\n 'number': 348,\n 'template': 349,\n 'important': 350,\n 'saying': 351,\n 'pov': 352,\n 'written': 353,\n 'always': 354,\n 'true': 355,\n 'media': 356,\n 'further': 357,\n 'oh': 358,\n '4': 359,\n 'term': 360,\n '5': 361,\n 'hate': 362,\n 'quite': 363,\n 'perhaps': 364,\n 'states': 365,\n 'until': 366,\n 'bit': 367,\n 'whether': 368,\n 'review': 369,\n 'consider': 370,\n 'claim': 371,\n 'guidelines': 372,\n 'fucking': 373,\n 'research': 374,\n 'once': 375,\n 'version': 376,\n 'based': 377,\n 'criteria': 378,\n 'times': 379,\n 'nigger': 380,\n 'website': 381,\n 'suck': 382,\n 'getting': 383,\n 'mention': 384,\n 'three': 385,\n 'makes': 386,\n 'several': 387,\n 'considered': 388,\n 'title': 389,\n 'words': 390,\n 'c': 391,\n 'hey': 392,\n 'year': 393,\n 'changes': 394,\n 'idea': 395,\n 'cannot': 396,\n 'ass': 397,\n 'address': 398,\n 'notice': 399,\n 'possible': 400,\n 'left': 401,\n 'current': 402,\n 'following': 403,\n 'group': 404,\n 'listed': 405,\n 'each': 406,\n 'second': 407,\n 'means': 408,\n 'date': 409,\n 'facts': 410,\n 'rules': 411,\n 'general': 412,\n 'care': 413,\n 'regarding': 414,\n 'main': 415,\n 'american': 416,\n 'man': 417,\n 'start': 418,\n 'mentioned': 419,\n 'course': 420,\n 'attack': 421,\n 'topic': 422,\n 'kind': 423,\n 'whole': 424,\n 'statement': 425,\n 'known': 426,\n 'include': 427,\n 'seen': 428,\n 'create': 429,\n 'end': 430,\n 'issues': 431,\n '10': 432,\n 'less': 433,\n 'gay': 434,\n 'related': 435,\n 'call': 436,\n 'ok': 437,\n 'sense': 438,\n 'big': 439,\n 'suggest': 440,\n 'happy': 441,\n 'including': 442,\n 'notability': 443,\n 'provide': 444,\n 'category': 445,\n 'days': 446,\n 'redirect': 447,\n 'myself': 448,\n 'move': 449,\n 'sentence': 450,\n \"wikipedia's\": 451,\n '2005': 452,\n 'info': 453,\n 'love': 454,\n 't': 455,\n 'four': 456,\n 'jpg': 457,\n 'appropriate': 458,\n 'contribs': 459,\n 'school': 460,\n 'changed': 461,\n 'neutral': 462,\n 'project': 463,\n 'explain': 464,\n 'started': 465,\n 'line': 466,\n 'mind': 467,\n 'anyway': 468,\n 'included': 469,\n 'removing': 470,\n 'looking': 471,\n 'next': 472,\n 'picture': 473,\n 'specific': 474,\n 'although': 475,\n 'per': 476,\n 'interest': 477,\n 'community': 478,\n 'relevant': 479,\n 'order': 480,\n 'die': 481,\n 'sign': 482,\n 'answer': 483,\n 'away': 484,\n 'warning': 485,\n 'lol': 486,\n 'summary': 487,\n 'full': 488,\n 'recent': 489,\n 'later': 490,\n 'faith': 491,\n 'policies': 492,\n 'claims': 493,\n 'discuss': 494,\n 'currently': 495,\n 'attacks': 496,\n 'wrote': 497,\n 'writing': 498,\n 'public': 499,\n 'especially': 500,\n 'interested': 501,\n 'able': 502,\n 'wish': 503,\n 'taken': 504,\n 'file': 505,\n 'position': 506,\n 'names': 507,\n 'within': 508,\n 'single': 509,\n 'below': 510,\n 'stuff': 511,\n 'during': 512,\n '6': 513,\n 'wanted': 514,\n 'appears': 515,\n 'official': 516,\n 'certainly': 517,\n 'live': 518,\n 'nice': 519,\n 'self': 520,\n '2006': 521,\n 'itself': 522,\n 'color': 523,\n 'everyone': 524,\n 'background': 525,\n 'anti': 526,\n 'country': 527,\n 'lead': 528,\n '20': 529,\n 'web': 530,\n 'common': 531,\n 'report': 532,\n 'unless': 533,\n 'god': 534,\n 'high': 535,\n 'completely': 536,\n 'according': 537,\n '0': 538,\n 'news': 539,\n 'pretty': 540,\n 'hard': 541,\n 'everything': 542,\n 'due': 543,\n 'published': 544,\n 'process': 545,\n 'edited': 546,\n 'looks': 547,\n 'involved': 548,\n '7': 549,\n '24': 550,\n 'fat': 551,\n 'therefore': 552,\n 'remember': 553,\n 'obviously': 554,\n 'power': 555,\n 'nor': 556,\n 'future': 557,\n 'd': 558,\n 'truth': 559,\n 'p': 560,\n 'came': 561,\n 'response': 562,\n 'reading': 563,\n 'sandbox': 564,\n '100': 565,\n 'party': 566,\n 'stay': 567,\n 'past': 568,\n 'learn': 569,\n 'game': 570,\n 'admins': 571,\n 'quote': 572,\n 'asked': 573,\n 'b': 574,\n 'stupid': 575,\n 'entry': 576,\n 'posted': 577,\n '11': 578,\n 'city': 579,\n 'faggot': 580,\n 'whatever': 581,\n 'talking': 582,\n 'ago': 583,\n 'placed': 584,\n 'political': 585,\n 'similar': 586,\n 'system': 587,\n 'today': 588,\n 'administrator': 589,\n 'argument': 590,\n 'paragraph': 591,\n 'exactly': 592,\n 'working': 593,\n 'guy': 594,\n '8': 595,\n 'false': 596,\n 'united': 597,\n 'took': 598,\n 'useful': 599,\n 'british': 600,\n 'noticed': 601,\n 'moron': 602,\n 'government': 603,\n 'regards': 604,\n '2007': 605,\n 'small': 606,\n 'reasons': 607,\n 'books': 608,\n 'form': 609,\n 'side': 610,\n 'dispute': 611,\n 'deleting': 612,\n '12': 613,\n 'guess': 614,\n 'five': 615,\n 'appreciate': 616,\n 'particular': 617,\n 'national': 618,\n 'reverting': 619,\n 'major': 620,\n 'problems': 621,\n 'bitch': 622,\n 'npov': 623,\n '2008': 624,\n '000': 625,\n 'rule': 626,\n 'banned': 627,\n '15': 628,\n 'often': 629,\n 'law': 630,\n 'provided': 631,\n 'become': 632,\n 'needed': 633,\n 'search': 634,\n 'reply': 635,\n 'wikiproject': 636,\n 'tried': 637,\n 'almost': 638,\n 'knowledge': 639,\n 'along': 640,\n 'cheers': 641,\n 'soon': 642,\n 'stated': 643,\n 'username': 644,\n 'status': 645,\n 'music': 646,\n 'taking': 647,\n 'fine': 648,\n 'film': 649,\n 'com': 650,\n 'vandalize': 651,\n 'company': 652,\n 'present': 653,\n 'certain': 654,\n '9': 655,\n 'white': 656,\n 'follow': 657,\n 'otherwise': 658,\n 'sort': 659,\n 'terms': 660,\n 'explanation': 661,\n 'uploaded': 662,\n 'points': 663,\n 'google': 664,\n 'generally': 665,\n 'description': 666,\n 'recently': 667,\n 'entire': 668,\n 'open': 669,\n 'tags': 670,\n 'alone': 671,\n 'citation': 672,\n 'cited': 673,\n 'likely': 674,\n 'ban': 675,\n 'short': 676,\n 'aware': 677,\n 'g': 678,\n 'shows': 679,\n 'definition': 680,\n 'saw': 681,\n 'class': 682,\n '14': 683,\n 'indeed': 684,\n 'set': 685,\n 'week': 686,\n 'type': 687,\n 'mr': 688,\n 'decide': 689,\n 'cite': 690,\n 'views': 691,\n 'appear': 692,\n 'band': 693,\n 'simple': 694,\n 'family': 695,\n 'guys': 696,\n 'area': 697,\n 'contact': 698,\n 'contributing': 699,\n 'external': 700,\n 'theory': 701,\n 'piece': 702,\n 'interesting': 703,\n '2004': 704,\n 'test': 705,\n 'unblock': 706,\n 'actual': 707,\n 'story': 708,\n 'improve': 709,\n 'internet': 710,\n 'jew': 711,\n 'copy': 712,\n 'sourced': 713,\n 'told': 714,\n 'attention': 715,\n 'email': 716,\n 'proposed': 717,\n 'obvious': 718,\n 'moved': 719,\n '16': 720,\n 'various': 721,\n 'allowed': 722,\n 'members': 723,\n 'conflict': 724,\n 'themselves': 725,\n 'context': 726,\n \"article's\": 727,\n 'black': 728,\n 'result': 729,\n 'thus': 730,\n 'author': 731,\n 'disagree': 732,\n 'cunt': 733,\n 'university': 734,\n 'went': 735,\n 'citations': 736,\n 'jews': 737,\n 'john': 738,\n 'ones': 739,\n 'hand': 740,\n 'actions': 741,\n 'bias': 742,\n 'previous': 743,\n 'third': 744,\n 'hours': 745,\n 'sites': 746,\n 'nonsense': 747,\n 'human': 748,\n 'works': 749,\n 'enjoy': 750,\n 'death': 751,\n 'job': 752,\n 'proper': 753,\n '18': 754,\n 'science': 755,\n 'together': 756,\n 'longer': 757,\n 'large': 758,\n '17': 759,\n 'sucks': 760,\n 'addition': 761,\n 'creating': 762,\n 'avoid': 763,\n 'happened': 764,\n 'valid': 765,\n 'jewish': 766,\n '13': 767,\n '19': 768,\n 'automatically': 769,\n 'biased': 770,\n 'german': 771,\n 'proof': 772,\n 'deal': 773,\n 'worked': 774,\n 'himself': 775,\n 'dick': 776,\n 'seriously': 777,\n 'goes': 778,\n 'level': 779,\n 'series': 780,\n '23': 781,\n 'accepted': 782,\n '21': 783,\n 'standard': 784,\n 'respect': 785,\n 'exist': 786,\n 'available': 787,\n 'helpful': 788,\n 'comes': 789,\n 'manual': 790,\n 'meaning': 791,\n 'opinions': 792,\n 'living': 793,\n 'sex': 794,\n 'criticism': 795,\n 'rights': 796,\n 'tildes': 797,\n 'act': 798,\n 'action': 799,\n 'play': 800,\n 'necessary': 801,\n 'accept': 802,\n 'indicate': 803,\n 'sections': 804,\n 'personally': 805,\n 'calling': 806,\n 'video': 807,\n 'yeah': 808,\n 'violation': 809,\n 'accurate': 810,\n 'statements': 811,\n 'pig': 812,\n 'hell': 813,\n 'months': 814,\n 'attempt': 815,\n 'july': 816,\n 'assume': 817,\n 'afd': 818,\n 'upon': 819,\n '2009': 820,\n 'historical': 821,\n 'usually': 822,\n 'debate': 823,\n 'f': 824,\n '30': 825,\n 'pro': 826,\n '22': 827,\n 'rest': 828,\n 'multiple': 829,\n 'blocking': 830,\n 'tagged': 831,\n 'serious': 832,\n 'width': 833,\n 'doubt': 834,\n 'record': 835,\n 'details': 836,\n 'greek': 837,\n 'de': 838,\n 'south': 839,\n 'm': 840,\n 'fix': 841,\n 'separate': 842,\n 'situation': 843,\n '2010': 844,\n 'speak': 845,\n 'space': 846,\n 'heard': 847,\n 'refer': 848,\n 'explaining': 849,\n 'okay': 850,\n 'quality': 851,\n 'church': 852,\n 'run': 853,\n 'penis': 854,\n 'complete': 855,\n 'none': 856,\n 'august': 857,\n 'messages': 858,\n 'sock': 859,\n 'asking': 860,\n 'lack': 861,\n 'data': 862,\n 'period': 863,\n 'legal': 864,\n 'rationale': 865,\n 'prove': 866,\n 'behavior': 867,\n 'apparently': 868,\n 'freedom': 869,\n 'uk': 870,\n 'team': 871,\n 'india': 872,\n 'online': 873,\n 'access': 874,\n 'changing': 875,\n 'bullshit': 876,\n 'march': 877,\n 'close': 878,\n 'military': 879,\n 'directly': 880,\n 'difference': 881,\n 'contribute': 882,\n 'culture': 883,\n 'early': 884,\n 'box': 885,\n 'existing': 886,\n 'wikipedian': 887,\n 'huge': 888,\n 'couple': 889,\n 'gets': 890,\n 'among': 891,\n 'supposed': 892,\n 'head': 893,\n 'primary': 894,\n 'except': 895,\n 'warring': 896,\n 'countries': 897,\n 'civil': 898,\n 'meant': 899,\n 'modern': 900,\n 'born': 901,\n '50': 902,\n 'special': 903,\n 'incorrect': 904,\n 'described': 905,\n 'uses': 906,\n 'significant': 907,\n 'fish': 908,\n 'disruptive': 909,\n '25': 910,\n 'field': 911,\n 'specifically': 912,\n 'purpose': 913,\n 'red': 914,\n 'pillars': 915,\n 'photo': 916,\n 'friend': 917,\n 'million': 918,\n 'produce': 919,\n 'r': 920,\n 'error': 921,\n 'june': 922,\n 'earlier': 923,\n 'force': 924,\n 'table': 925,\n 'computer': 926,\n 'release': 927,\n 'sometimes': 928,\n 'half': 929,\n 'outside': 930,\n 'vote': 931,\n 'january': 932,\n 'inclusion': 933,\n 'particularly': 934,\n 'cases': 935,\n 'business': 936,\n 'character': 937,\n 'pictures': 938,\n 'linked': 939,\n 'gave': 940,\n 'abuse': 941,\n 'possibly': 942,\n 'control': 943,\n 'shall': 944,\n 'numbers': 945,\n 'tv': 946,\n 'anonymous': 947,\n 'member': 948,\n 'scientific': 949,\n 'arguments': 950,\n 'christian': 951,\n 'tutorial': 952,\n 'border': 953,\n 'reported': 954,\n 'idiot': 955,\n 'eat': 956,\n 'wait': 957,\n 'coming': 958,\n 'creation': 959,\n 'song': 960,\n 'thinking': 961,\n 'house': 962,\n 'happen': 963,\n 'concerns': 964,\n 'majority': 965,\n 'contest': 966,\n 'giving': 967,\n 'allow': 968,\n 'n': 969,\n 'decision': 970,\n 'takes': 971,\n 'bring': 972,\n \"'\": 973,\n 'home': 974,\n 'readers': 975,\n 'worth': 976,\n 'kill': 977,\n 'friends': 978,\n 'o': 979,\n 'groups': 980,\n 'north': 981,\n 'align': 982,\n 'totally': 983,\n 'finally': 984,\n 'discussed': 985,\n 'decided': 986,\n 'putting': 987,\n 'mistake': 988,\n 'administrators': 989,\n 'lost': 990,\n 'absolutely': 991,\n 'april': 992,\n 'respond': 993,\n 'international': 994,\n 'asshole': 995,\n 'independent': 996,\n '27': 997,\n 'archive': 998,\n 'standards': 999,\n 'towards': 1000,\n ...}"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"**A note here is although our vocab_size is setted to 20000, the tokenizer still keeps 192395 in the word_index.\nword_index is computed the same way no matter how many most frequent words you will use later. So when you call any transformative method - Tokenizer will use only the vocab_size most common words and at the same time, it will keep the counter of all words - even when it's obvious that it will not use it later.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train = tokenizer.texts_to_sequences(x_train)\nx_test = tokenizer.texts_to_sequences(x_test)","execution_count":11,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Add padding"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.sequence import pad_sequences\n\nmaxlen = 150\nx_train = pad_sequences(x_train, padding='post', maxlen=maxlen)\nx_test = pad_sequences(x_test, padding='post', maxlen=maxlen)","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x[0]","execution_count":13,"outputs":[{"output_type":"execute_result","execution_count":13,"data":{"text/plain":"'Explanation\\nWhy the edits made under my username Hardcore Metallica Fan were reverted? They were not vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please do not remove the template from the talk page since I am retired now.89.205.38.27'"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"example = tokenizer.texts_to_sequences([x[0]])\nexample = pad_sequences(example, padding='post', maxlen=maxlen)\nexample","execution_count":14,"outputs":[{"output_type":"execute_result","execution_count":14,"data":{"text/plain":"array([[  661,    75,     1,   123,   127,   172,    29,   644,  4448,\n        11794,  1068,    83,   313,    48,    83,    10, 11223,    53,\n         6775,    15,    60,  2699,   144,     3,  2863,    36,   114,\n         1176, 15683,  2756,     5,    47,    20,    10,   232,     1,\n          349,    32,     1,    40,    28,   140,     3,    37,  3372,\n           87,  3003,  4523,  2235,   997,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0]], dtype=int32)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer.sequences_to_texts(example)","execution_count":15,"outputs":[{"output_type":"execute_result","execution_count":15,"data":{"text/plain":"['explanation why the edits made under my username hardcore metallica fan were reverted they were not vandalisms just closure on some gas after i voted at new york dolls fac and please do not remove the template from the talk page since i am retired now 89 205 38 27']"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"# Load pretrained embedding"},{"metadata":{"trusted":true},"cell_type":"code","source":"embeddings_dictionary = dict()\n\nglove_file = open('../input/glove6b100dtxt/glove.6B.100d.txt', encoding=\"utf8\")\n\nfor line in glove_file:\n    records = line.split()\n    word = records[0]\n    vector_dimensions = np.asarray(records[1:], dtype='float32')\n    embeddings_dictionary[word] = vector_dimensions\nglove_file.close()\n\n\n\ndef create_glove(word_index,embeddings_index):\n    emb_mean,emb_std = -0.005838499,0.48782197\n    all_embs = np.stack(embeddings_index.values())\n    embed_size = all_embs.shape[1]\n    nb_words = min(vocab_size, len(word_index))\n    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n    count_found = nb_words\n    for word, i in word_index.items():\n        if i >= vocab_size: continue\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None:\n            embedding_matrix[i] =  embedding_vector\n        else:\n                count_found-=1\n    print(\"Got embedding for \",count_found,\" words.\")\n    return embedding_matrix\n","execution_count":33,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_matrix = create_glove(tokenizer.word_index,embeddings_dictionary)","execution_count":34,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3331: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n  exec(code_obj, self.user_global_ns, self.user_ns)\n","name":"stderr"},{"output_type":"stream","text":"Got embedding for  18783  words.\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"all(embedding_matrix[1] == embeddings_dictionary['the'])","execution_count":37,"outputs":[{"output_type":"execute_result","execution_count":37,"data":{"text/plain":"True"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"all(embedding_matrix[tokenizer.word_index['which']] == embeddings_dictionary['which'])","execution_count":38,"outputs":[{"output_type":"execute_result","execution_count":38,"data":{"text/plain":"True"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"# Model building with loaded pretrained embeddings"},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\nfrom tensorflow.keras.layers import Bidirectional, GlobalMaxPool1D\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras import initializers, regularizers, constraints, optimizers, layers\nimport tensorflow_addons as tfa\n\ndef model_add():\n    inputs = Input(shape=(maxlen, ))\n    x = Embedding(vocab_size, 100, weights=[embedding_matrix], trainable=False)(inputs)\n    x = Bidirectional(LSTM(50))(x)\n    x = Dropout(0.1)(x)\n    x = Dense(50, activation=\"relu\")(x)\n    x = Dropout(0.1)(x)\n    outputs = Dense(6, activation=\"sigmoid\")(x)\n    model = Model(inputs=inputs, outputs=outputs)\n    #f1 = tfa.metrics.F1Score(num_classes=2,average = 'micro')\n    model.compile(loss='binary_crossentropy',\n                  optimizer='adam',\n                  metrics=['acc'])\n    return model\nmodel_w_glove = model_add()\nprint(model_w_glove.summary())","execution_count":44,"outputs":[{"output_type":"stream","text":"Model: \"functional_3\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_2 (InputLayer)         [(None, 150)]             0         \n_________________________________________________________________\nembedding_1 (Embedding)      (None, 150, 100)          2000000   \n_________________________________________________________________\nbidirectional_1 (Bidirection (None, 100)               60400     \n_________________________________________________________________\ndropout_2 (Dropout)          (None, 100)               0         \n_________________________________________________________________\ndense_2 (Dense)              (None, 50)                5050      \n_________________________________________________________________\ndropout_3 (Dropout)          (None, 50)                0         \n_________________________________________________________________\ndense_3 (Dense)              (None, 6)                 306       \n=================================================================\nTotal params: 2,065,756\nTrainable params: 65,756\nNon-trainable params: 2,000,000\n_________________________________________________________________\nNone\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.callbacks import EarlyStopping, ModelCheckpoint\nes = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=20)\nhistory = model_w_glove.fit(x_train, y_train, batch_size=32, epochs=5, validation_data=(x_test,y_test), callbacks=es)","execution_count":45,"outputs":[{"output_type":"stream","text":"Epoch 1/5\n3990/3990 [==============================] - 352s 88ms/step - loss: 0.0665 - acc: 0.9343 - val_loss: 0.0535 - val_acc: 0.9940\nEpoch 2/5\n3990/3990 [==============================] - 344s 86ms/step - loss: 0.0528 - acc: 0.9601 - val_loss: 0.0499 - val_acc: 0.9939\nEpoch 3/5\n3990/3990 [==============================] - 347s 87ms/step - loss: 0.0487 - acc: 0.9810 - val_loss: 0.0491 - val_acc: 0.9929\nEpoch 4/5\n3990/3990 [==============================] - 343s 86ms/step - loss: 0.0456 - acc: 0.9847 - val_loss: 0.0475 - val_acc: 0.9920\nEpoch 5/5\n3990/3990 [==============================] - 343s 86ms/step - loss: 0.0434 - acc: 0.9770 - val_loss: 0.0474 - val_acc: 0.9940\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred2 = model_w_glove.predict(x_test)\ny_pred2 = y_pred2 > 0.5\nfrom sklearn.metrics import f1_score\nf1_score(y_test, y_pred2, average='micro')","execution_count":42,"outputs":[{"output_type":"execute_result","execution_count":42,"data":{"text/plain":"0.7492077863286555"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred3 = model_w_glove.predict(x_train)\ny_pred3 = y_pred3 > 0.5\nfrom sklearn.metrics import f1_score\nf1_score(y_train, y_pred3, average='micro')","execution_count":43,"outputs":[{"output_type":"execute_result","execution_count":43,"data":{"text/plain":"0.7818553268765134"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"# Model building without pretrained embeddings"},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\nfrom tensorflow.keras.layers import Bidirectional, GlobalMaxPool1D\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras import initializers, regularizers, constraints, optimizers, layers\nimport tensorflow_addons as tfa\n\ndef model_add():\n    inputs = Input(shape=(maxlen, ))\n    x = Embedding(vocab_size, 128)(inputs)\n    x = Bidirectional(LSTM(50))(x)\n    x = Dropout(0.1)(x)\n    x = Dense(50, activation=\"relu\")(x)\n    x = Dropout(0.1)(x)\n    outputs = Dense(6, activation=\"sigmoid\")(x)\n    model = Model(inputs=inputs, outputs=outputs)\n    #f1 = tfa.metrics.F1Score(num_classes=2,average = 'micro')\n    model.compile(loss='binary_crossentropy',\n                  optimizer='adam',\n                  metrics=['acc'])\n    return model\nmodel = model_add()\nprint(model.summary())","execution_count":29,"outputs":[{"output_type":"stream","text":"Model: \"functional_5\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_3 (InputLayer)         [(None, 120)]             0         \n_________________________________________________________________\nembedding_2 (Embedding)      (None, 120, 128)          2560000   \n_________________________________________________________________\nbidirectional_2 (Bidirection (None, 100)               71600     \n_________________________________________________________________\ndropout_4 (Dropout)          (None, 100)               0         \n_________________________________________________________________\ndense_4 (Dense)              (None, 50)                5050      \n_________________________________________________________________\ndropout_5 (Dropout)          (None, 50)                0         \n_________________________________________________________________\ndense_5 (Dense)              (None, 6)                 306       \n=================================================================\nTotal params: 2,636,956\nTrainable params: 2,636,956\nNon-trainable params: 0\n_________________________________________________________________\nNone\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.callbacks import EarlyStopping, ModelCheckpoint\nes = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=20)\nmodel.fit(x_train, y_train, batch_size=32, epochs=5, validation_data=(x_test,y_test), callbacks=es)","execution_count":30,"outputs":[{"output_type":"stream","text":"Epoch 1/5\n3990/3990 [==============================] - 396s 99ms/step - loss: 0.0643 - f1_score: 0.1554 - val_loss: 0.0488 - val_f1_score: 0.1568\nEpoch 2/5\n3990/3990 [==============================] - 393s 99ms/step - loss: 0.0455 - f1_score: 0.1571 - val_loss: 0.0487 - val_f1_score: 0.1568\nEpoch 3/5\n3990/3990 [==============================] - 393s 99ms/step - loss: 0.0390 - f1_score: 0.1570 - val_loss: 0.0488 - val_f1_score: 0.1568\nEpoch 4/5\n3990/3990 [==============================] - 394s 99ms/step - loss: 0.0330 - f1_score: 0.1572 - val_loss: 0.0553 - val_f1_score: 0.1564\nEpoch 5/5\n3990/3990 [==============================] - 394s 99ms/step - loss: 0.0283 - f1_score: 0.1577 - val_loss: 0.0573 - val_f1_score: 0.1559\n","name":"stdout"},{"output_type":"execute_result","execution_count":30,"data":{"text/plain":"<tensorflow.python.keras.callbacks.History at 0x7efd351cb810>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"* issue:\ntensorflow addon's f1 score seems not working well. Either not working with multi-label problems or because it's not a streaming metric. Need futher check."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}